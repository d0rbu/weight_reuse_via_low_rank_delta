# num_layers_and_model_name: [40, "meta-llama/Llama-2-13b-hf"]
num_layers_and_model_name: [32, "meta-llama/Llama-2-7b-hf"]
param_name_combinations:
  - ["self_attn.q_proj"]
  - ["self_attn.k_proj"]
  - ["self_attn.q_proj", "self_attn.k_proj"]
num_weight_groups: 32  # n_heads in our model
base_layers: 0
move_device: "cuda:0"
tasks: ["winogrande", "wikitext"]